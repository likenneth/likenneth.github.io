<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kenneth Li</title>
  
  <meta name="author" content="Kenneth Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/paperclip.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kenneth Li</name>
              </p>
              <p>
                I am a final-year PhD student at Harvard, advised by <a href="https://www.bewitched.com/">Martin Wattenberg</a>, <a href="http://www.fernandaviegas.com/">Fernanda Viégas</a>, and <a href="https://vcg.seas.harvard.edu/people/hanspeter-pfister/">Hanspeter Pfister</a>. I am funded by <a href="https://www.harvard.edu/kempner-institute/">Kempner Institute Graduate Fellowship</a>. I hold a graduate-student <a href="https://openai.com/blog/superalignment-fast-grants">Superalignment Fast Grant</a> from OpenAI. I interned at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSR Asia</a> and <a href="https://ai.facebook.com/">Meta AI</a>.
              </p>

              <p>
                I aim at understanding the inner workings of large language models and, based on these findings, control them for improved human-AI collaboration. Feel free to reach out if you'd like to discuss!
              </p>
              <p style="text-align:center">

              </p>
              <p style="text-align:center">
                Contact: ke_li [at] g.harvard.edu <br>
<!--                Address: 150 Western Ave, Allston, MA 02134 <br>-->
<!--                <a href="mailto:ke_li@g.harvard.edu">Email</a> &nbsp|&nbsp-->
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp|&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp|nbsp-->
                <a href="https://scholar.google.com/citations?user=v0GItgwAAAAJ&hl">Google Scholar</a> &nbsp|&nbsp
                <a href="https://twitter.com/ke_li_2021">Twitter</a> &nbsp|&nbsp
                <a href="https://github.com/likenneth">GitHub</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/kenneth-li-103072201/">LinkedIn</a>
                <!-- <a href="https://forms.gle/2RLQNno3Jnshgvwm9">Anonymous Feedback Form</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <!-- <a href="images/KennethLi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/KennethLi_circle.jpg" class="hoverZoomLink"></a> -->
              <a href="images/kl_circled.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/kl_circled.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Research</heading>
<!--              <p>-->
<!--                Representative papers are <span class="highlight">highlighted</span>.-->
<!--              </p>-->
            </td>
          </tr>
        </tbody></table>

        <!-- Add border="1" here to debug-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2406.11978.pdf">
                <papertitle>Dialogue Action Tokens: Steering Language Models in Goal-Directed Dialogue with a Multi-Turn Planner</papertitle>
              </a>
              <br>
              <strong>Kenneth Li</strong>*, Yiming Wang*, Fernanda Viégas, Martin Wattenberg
              <br>
              preprint
              <br>
              <a href="https://arxiv.org/abs/2406.11978">Arxiv</a> | <a href="https://github.com/likenneth/dialogue_action_token">Code</a>
              <br>A small planner model is trained using reinforcement learning to steer large language models in multi-round dialogues.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2406.07882.pdf">
                <papertitle>Designing a Dashboard for Transparency and Control of Conversational AI</papertitle>
              </a>
              <br>
              Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, <strong>Kenneth Li</strong>, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Fernanda Viégas, Martin Wattenberg
              <br>
              preprint
              <br>
              <a href="https://arxiv.org/abs/2406.07882">Arxiv</a> | <a href="https://github.com/yc015/TalkTuner-chatbot-llm-dashboard/tree/main">Code</a> | <a href="https://yc015.github.io/TalkTuner-a-dashboard-ui-for-chatbot-llm/">Project Page</a>
              <br>We design and evaluate a dashboard interface for visualizing and controlling the internal user model in a conversational LLM.
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2402.10962.pdf">
                <papertitle>Measuring and Controlling Instruction (In)Stability in Language Model Dialogs</papertitle>
              </a>
              <br>
              <strong>Kenneth Li</strong>, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
              <br>
              <em>COLM</em>, 2024 <font color="crimson"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2402.10962">Arxiv</a> | <a href="https://github.com/likenneth/persona_drift">Code</a> | <a href="https://thegradient.pub/dialog/">The Gradient</a>
              <br>When a dialogue goes long, a chatbot ceases to follow its system prompt surprisingly quickly—within 8 rounds.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2402.14688.pdf">
                <papertitle>Q-Probe: A Lightweight Approach to Reward Maximization for Language Models</papertitle>
              </a>
              <br>
              <strong>Kenneth Li</strong>, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener 
              <br>
              <em>ICML</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2402.14688">Arxiv</a> | <a href="https://github.com/likenneth/q_probe">Code</a>
              <br>Through rejection sampling, we leverage a language model's own discriminative capability to boost its generative capability.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2306.03341.pdf">
                <papertitle>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</papertitle>
              </a>
              <br>
              <strong>Kenneth Li</strong>*, Oam Patel*, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
              <br>
              <em>NeurIPS</em>, 2023 <font color="crimson"><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.03341">Arxiv</a> | <a href="https://github.com/likenneth/honest_llama">Code</a>  | <a href="https://huggingface.co/likenneth/honest_llama2_chat_7B">Stand-alone Model</a> 
              <br>By manipulating the activations of a language model, we can compel it to tell the truth it knows but otherwise hides.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2210.13382.pdf">
                <papertitle>Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</papertitle>
              </a>
              <br>
              <strong>Kenneth Li</strong>, Aspen Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
              <br>
              <em>ICLR</em>, 2023 <font color="crimson"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2210.13382">Arxiv</a> | <a href="https://github.com/likenneth/othello_world">Code</a> | <a href="othello/togglable.html">Demo</a> | <a href="https://thegradient.pub/othello/">The Gradient</a> | <a href="https://www.scientificamerican.com/article/how-ai-knows-things-no-one-told-it/"> Scientific American</a> | <a href="https://www.theatlantic.com/magazine/archive/2023/09/sam-altman-openai-chatgpt-gpt-4/674764/?utm_source=feed"> The Atlantic</a> | <a href="https://www.nature.com/articles/d41586-023-02361-7#:~:text=Argument%20for%20reasoning"> Nature News</a> | <a href="https://www.deeplearning.ai/the-batch/issue-209/"> Andrew Ng</a>
              <br>In a transformer trained on Othello transcripts, we uncover an interpretable and controllable world model of the game board. 
              <p></p>
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2111.11433.pdf">
                <papertitle>Towards Tokenized Human Dynamics Representation</papertitle>
              </a>
              <br>
              <strong>Kenneth Li</strong>, Xiao Sun, Zhirong Wu, Fangyun Wei, Stephen Lin
              <br>
              preprint
              <br>
              <a href="https://arxiv.org/abs/2111.11433">Arxiv</a> | <a href="https://github.com/likenneth/acton">Code</a> | <a href="https://drive.google.com/file/d/1JJpc-WzrggQU8xT8TZLdW2ytW5Xk090d/view">Demo</a>
              <br>By identifying recurring temporal patterns without supervision, we transform video understanding into a language problem.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2104.06976.pdf">
                <papertitle>Pose Recognition with Cascade Transformers</papertitle>
              </a>
              <br>
              <strong>Ke Li</strong>*, Shijie Wang*, Xiang Zhang*, Yifan Xu, Weijian Xu, Zhuowen Tu
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2104.06976">Arxiv</a> | <a href="https://github.com/mlpc-ucsd/PRTR">Code</a> | <a href="https://0633e92166c0a27ea1aa-ab47878a9e45eb9e2f15be38a59f867e.ssl.cf1.rackcdn.com/TFCCYRWN-1700030-1332322-Upload-1622494976.mp4">Video</a>
              <br>Using Transformer, we clean up heuristic designs that have long bedeviled pose estimation models in an end-to-end fashion.
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:8px;width:1%;vertical-align:middle">
              <img src="images/white.jpg" alt="clean-usnob" width="1" height="1">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2003.13962.pdf">
                <papertitle>Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text</papertitle>
              </a>
              <br>
              Difei Gao*, <strong>Ke Li</strong>*, Ruiping Wang, Shiguang Shan, Xilin Chen
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2003.13962">Arxiv</a> | <a href="https://github.com/likenneth/mmgnn_textvqa">Code</a> | <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Gao_Multi-Modal_Graph_Neural_CVPR_2020_supplemental.pdf">Supp</a> | <a href="https://www.youtube.com/watch?v=Sw1s8LWg1ss">Video</a>
              <br>Texts found in everyday images can be rare and polysemous; we pin down their semantics by cross-modality message passing.
              <p></p>
            </td>
          </tr> -->

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                <!-- Template from <a href="https://https://jonbarron.info/">Jon Barron's website</a> -->
                <br>
                Latest update: Jan 2025
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
